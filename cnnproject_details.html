<!DOCTYPE html>
<html lang="en">
<head>
<title>Maggie's Codes</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
*{
    box-sizing: border-box;
}

body {
    margin: 0;
}

/* Style the header */
.header {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 25px;
    font-weight: bold;
    letter-spacing: 1.4px;
    background-color: #f1f1f1;
    padding: 10px;
    text-align: center;
}

H2 {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: normal;
    letter-spacing: 1.2px;
    color: #009900;
}
tab2 { 
    padding-left: 2em; 
}
P {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 15px;
    color: #555555;
    line-height: 1.5;
    letter-spacing: .25px;
}
/* Style the top navigation bar */
.topnav {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: normal;
    letter-spacing: 1.2px;
    color: #000000;
    overflow: hidden;
    background-color: #f9f9f9;
}

/* Style the topnav links */
.topnav a {
    float: left;
    display: block;
    color: #000000;
    text-align: center;
    padding: 14px 16px;
    text-decoration: none;
}

/* Change color on hover */
.topnav a:hover {
    background-color: #ddd;
    color: #009900;
}
/* Create three unequal columns that floats next to each other */
.column {
    float: left;
    padding: 10px;
}

/* Left and right column */
.column.side {
    width: 15%;
}

/* Middle column */
.column.middle {
    width: 70%;
}

/* Clear floats after the columns */
.row:after {
    content: "";
    display: table;
    clear: both;
}

/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media (max-width: 800px) {
    .column.side, .column.middle {
        width: 100%;
    }
}

.dropbtn {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 12px;
    font-weight: normal;
    letter-spacing: 1.2px;
    background-color: #f9f9f9;
    border-bottom: solid 1px #f9f9f9;
    text-transform: uppercase;
    color: black;
    padding: 8px;
    font-size: 16px;
    border: none;
    cursor: pointer;
}

.dropdown {
    position: relative;
    display: inline-block;
}

.dropdown-content {
    display: none;
    position: absolute;
    background-color: #f9f9f9;
    min-width: 160px;
    box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.2);
    z-index: 1;
}

.dropdown-content a {
    font-family: Arial, Helvetica, sans-serif;
    font-size: 12px;
    font-weight: normal;
    letter-spacing: 1.2px;
    color: black;
    padding: 12px 16px;
    text-decoration: none;
    display: block;
}

.dropdown-content a:hover {background-color: #f1f1f1}

.dropdown:hover .dropdown-content {
    display: block;
}

.dropdown:hover .dropbtn {
    background-color: #3e8e41;
}

table, th, td {
    border: 1px solid black;
}

</style>
</head>
<body>

<div class="header">
  <h1>Programming: </h1>
<h2> "Our brains grow because the biological design of our camera-like eyes intake what we see and
sends the exact visual data to the visual cortex, where it stores an abstraction, or abstractions, of the
visual data effortlessly in a way that we can retrieve it without a knowing the location. It is stored and
retrieved dynamically, nonpermanently and nonforcefully. We learn by choosing between such
abstracts, which are triggered simultaneously through the interchange between the visual object in the
real world and the systems in our brain, precisely in the visual cortex. The visual cortex organizes
information dynamically through a set of random addresses, or lines of synaptic connections, where the
abstract data is processed somewhere in the visual cortex to become precise data. The beauty lies in the
fact that we do not have to explicitly define the abstractions of a visual object because the abstraction is
the nature of the system within the visual cortex." 
</h2>
</div>
<div class="topnav">
  <a href="index.html">Home</a>
  <a href="project_logs.html">Project Documentation</a>
  <a href="#">About</a>
  <a href="interests.html">Interests</a>
</div>

<div class="row">
  <div class="column side">
    <h2>Projects List</h2>
    <p>
<p><div class="dropdown">
  <button class="dropbtn">Simple Functions</button>
  <div class="dropdown-content">
    <a href="Basic_Functions/compare_sorting.cpp">Compare Sorting</a> 
    <a href="Basic_Functions/subset.cpp">Subset </a> 
    <a href="Basic_Functions/dynamic.cpp">Dynamic Function example</a>
  </div>
</div></p>

<p><div class="dropdown">
  <button class="dropbtn">Google Code Jam</button>
  <div class="dropdown-content">
    <a href="https://code.google.com/codejam/contest/544101/dashboard"> Assignment Link </a> 
    <a href="Google_Code_Jam/Board.cpp">Board Class</a> 
    <a href="Google_Code_Jam/Board.h">Board Header File</a> 
    <a href="Google_Code_Jam/mainBoard.cpp">Board main driver</a> 
    <a href="Google_Code_Jam/reverse.cpp">Reverse</a> <br>
    <a href="Google_Code_Jam/AlienLanguage.cpp">Alien Language</a>
    <a href="Google_Code_Jam/Numbers.cpp">Numbers</a>
    <a href="Google_Code_Jam/MinimumScalarProduct.cpp">Minimum Scalar Product</a>
  </div>
</div></p>

<p><div class="dropdown">
  <button class="dropbtn">Systems Programming</button>
  <div class="dropdown-content">
    <a href="Systems_Programming/uniq.c">Uniq command</a>
    <a href="Systems_Programming/runsim.c">Running child processes</a>
    <a href="Systems_Programming/my_grep.c">Grep command</a>
    <a href="Systems_Programming/pipe.c">Pipe example</a>
    <a href="Systems_Programming/my_nohup.c">Signals </a>
  </div>
</div><p>

<p><div class="dropdown">
  <button class="dropbtn">Mirai Project </button>
  <div class="dropdown-content">
    <a href="MiraiSecurityProject.pdf">Mirai execution report</a>
  </div>
</div><p>

<p><div class="dropdown">
  <button class="dropbtn">Simple Shapes CNN </button>
  <div class="dropdown-content">
    <a href="Simple_Shapes_CNN/proposal.pdf">Neural Network Concept</a>
    <a href="Simple_Shapes_CNN/draw.txt">Drawing random shapes</a>
    <a href="Simple_Shapes_CNN/pickletestcats.txt">Creating pickle files</a>
    <a href="Simple_Shapes_CNN/merge_files.txt">Merging pickle files</a>
    <a href="Simple_Shapes_CNN/load_merged_files.txt">Pickle loader to CNN</a>
    <a href="Simple_Shapes_CNN/cnn_copy_sobel_test.txt">CNN loader</a>
    <a href="Simple_Shapes_CNN/cnn_sobel_main.txt">Sobel Algorithm for Edge & Contour Detection</a>
    <a href="Simple_Shapes_CNN/save_pickle_images.txt">Reduce image size</a>
    <a href="Simple_Shapes_CNN/cnn_copy_sobel_test.txt">CNN loader</a>
    <a href="Simple_Shapes_CNN/test_model.txt">Test training model</a>
    <a href="Simple_Shapes_CNN/googlecloud_config_cnn.txt">Google Cloud Configuration</a> 
  </div>
</div><p>

</p>
</div>

  <div class="column middle">
  <div style="width:800px;height:700px;line-height:1.2em;overflow:auto;padding:0px;">
    <h2>Project Logs for Simple Shapes using Convolutional Neural Network project</h2>
    <p>
    <p><div class="dropdown">
  <button class="dropbtn">Project Logs</button>
  <div class="dropdown-content"> 
    <a href="Simple_Shapes_CNN/logs_text.pdf">Google Cloud Logs</a>
    <a href="Simple_Shapes_CNN/project_logs.pdf">Project Logs</a>
    <a href="Simple_Shapes_CNN/interim_status_report.pdf">Interim Status Report</a>
    <a href="Simple_Shapes_CNN/nn_architecture.pdf">CNN design & Research</a>
    <a href="Simple_Shapes_CNN/interim_status_report2.pdf">Interim Status Report II</a>
    <a href="sobel_crop.pdf">Sobel Algorithm Illustrations</a>
    <a href="Simple_Shapes_CNN/Final_Report.pdf">Project Final Report</a>
    <a href="Simple_Shapes_CNN/sobel_copy.pdf">Description of Project</a>
    <a href="Simple_Shapes_CNN/Project_logs_final.pdf">Virtual Machine (Ubuntu Xenial) Project Logs</a>
    <a href="Simple_Shapes_CNN/training_chart.pdf">Training Chart</a>
    <a href="Simple_Shapes_CNN/training_output_documentation.pdf">Training Output</a>
    </div>
  </div><p>
    </p>
  <h2>Single Output with Multi-Class Classification</h2>
<p>
<tab2> The convolutional neural network will learn using supervised training in Keras.
The training and validation datasets will have a supervised output of a single label 
for each class in a 300 by 300 image. The dataset consisting of circles, triangles, squares 
and rectangles is created by the program draw.py. It uses multiprocessing of independent pool workers 
to draw these shapes and saving these shapes in ".png" format. Draw.py uses the cairo library for drawing 
the shapes and the random library to randomize the location and size of these shapes. Each 300 by 300 
image will have two variations, which are the linear contours and the fill of the shape pertaining to 
the same class. To execute the file, simply run python draw.py --draw-func draw_circle --num-images 2000 
--file-dir training_set/circle/circle_, where draw-func represents the drawing function that draws 
either circle, triangle, square or rectangle or a combination of these shapes if needed. There are 
2,000 images per data shape with a total of 8,000 images for the training dataset. The validation 
dataset consists of 400 images per data shape with a total of 1,600 images, which is 20% of the 
training dataset. <br>
<tab2> A single PNG image file has a better pixel precision, but it comes at a cost of having 
twice or even three times as much data storage as a JPEG file. Since the training data tend 
to increase when the architecture of the convolutional neural network has more layers, it is efficient 
to reduce the size of the images by converting from PNG to JPEG. The program reduce_images.py 
reduces image sizes by adjusting a ratio of height and base decrease. It then saves the images in 
pickle file, where the same pickle file is opened to save the newly reduced size images into a folder. <br>
<tab2> After each image file size is reduced, pickletestcats.py is the main file that will process 
the reduced sized images. It is a multiprocessing program of independent pool workers, which uses the 
Pillow 3 module to open JPEG image files. The pool workers individually convert the JPEG image file into 
a numpy array of shape (300, 300, 3), where 300 represents the width and the height of the image and 3 
represents the color format. The image's corresponding output label (y_label) is generated by using the 
Keras library of to_categorical encoding. The encoding transforms integers into different binary 1 
and 0 encodings of output classes. These classes are circle, rectangle, triangle, and square. The numpy 
array's total size of 2000 images per shape is reduced by a half if the numpy data structure is a 
float-16 instead of float-32. After the execution of the program for each shape in the training and 
validation data, there will be eight pickle files called “circle.pkl”, “triangle.pkl”, “square.pkl”, 
“rectangle.pkl”, “valid_circle.pkl”, “valid_triangle.pkl”, “valid_square.pkl” and “valid_rectangle.pkl”. 
A tuple of (image data, output y label) is returned by the pool workers. <br>
<tab2> The combined pickled data of all the shapes represent the training dataset and the validation 
dataset, which is needed for the convolutional neural network to learn. The file merge_files_revised.py 
merges the shapes by using the Loader class, which is a multiprocessing module, to load the images and 
store them in a very large list. All the pool workers execute the loading of the images and the appending 
of the tuple (image data, output y label) independently. Therefore, a global storage of each large dataset 
list per shape is needed because each pool worker needs access to the it. I tried allocating the list using 
“None” times number of images. The numpy array is organized in a tuple, so the list allocation doesn't work 
in numpy arrays or in tuples. Memmap would not work either because the merged dataset contains a tuple, 
not a single memory array. After the loaded data is stored in the large lists, the program uses list concatenation 
to merge all the shapes together, forming the training and validation dataset. <br> 
<tab2> Since the dataset is a very large list, joblib does a successful job dumping and compressing into a file, 
which will be loaded from load_merge_files.py. The reason for having one file for merging the dataset and another 
file for loading the merged dataset is that it is better to shuffle the dataset before loading into the convolutional 
neural network. Although the Keras' generator automatically shuffles with each run of an epoch, it is a good practice 
to shuffle the training dataset to allow for randomization of images. The program shuffles the dataset and loads the 
previously merged files by organizing the tuple into two different lists. The first one contains the numpy pixel data 
and the second list contains its corresponding y label output, which is needed for a supervised training of the neural 
network. The large list is being stored using the numpy array's memory mapping because each shape of the data pixel 
array is (300, 300, 3) and its corresponding y output label is (4, ) where 4 represents the number of classes to be 
learned by the convolutional neural network. </p>
<h2>Background data without the Shape Contours</h2>
<p>
<tab2> It is redundant for the convolutional neural network in cnn_copy_sobel_test.py to obtain pixelized convolutions 
from background pixels that do not have object contours. An image preprocessing function is needed before the image 
data are inputted into the first two-dimensional convolution of the network. This function get_edges is responsible 
for cropping the image and reducing the background space. The input image needs to be converted into grayscale for 
the Sobel algorithm to process. Since findContours of the opencv2 module alters the original image, a copy of the 
input image is needed. After finding the contours in both the x and y directions of the Sobel algorithm, the gradients 
in both directions clarify the contour boundaries of the image. The combined gradient adds the weights in gradients 
from x and y derivatives, which finalizes the pixels for the image contours. <br>
<tab2>When the coordinates of the contours are detected, the rectangular boundaries are drawn around it by using opencv2’s rectangle method. A list for x coordinates and another list for y coordinates stores all the x and y coordinates returned by findContours. The minimum and maximum values of the x and y lists will determine the area of the cropping region. The locations of the rectangles are in (x,y) coordinates where the cropped version will be from the top left corner (smallest_x, smallest_y) and the bottom right opposite corner (largest_x, largest y). The original input image will be cropped by slicing the coordinates of the original array. <br>
<tab2>The layers of the convolutional neural network will gather pixels from images that are cropped and will not see the original image. To maintain the scale consistency of the size of the input image at 300 pixels by 300 pixels in RGB format, the cropped image needs to be resized in aspect ratio with a fixed width of 300 pixels. The flatten() layer of the convolutional neural network requires the input images to have the same numpy shape array; get_edges must return the same shape of (300, 300, 3). A white background of 300 pixels by 300 pixels is created and converted into an 8-bit unsigned integer numpy array. By using the paste function from the Pillow library, the resized cropped image is pasted onto the white background. The pasted, cropped image is converted back into a 32-bit numpy float value because Keras processes numpy arrays in 32-bit float values. </p>    
<h2>Convolutional Neural Network Architecture </h2>
<p> 
<tab2>This convolutional neural network architecture uses the Keras Sequential model that acts as instances being layered on top of each other. The convolutional layers in the beginning of the neural network gather pixels of each input data. Depending on the quality and quantity of the image data, the number of output filters will vary. The output filters are the third dimensionality of the convolutional neural layer at the z axis where a rise in output filters and convolutional layers increases the parameter of the network. Increasing the number of filters is necessary as the training data increases, so 128 filters for the first layer of the second convolutional neural network is sufficient for 8,000 JPEG images. The convolutional layer will operate for each 6 pixels by 6 pixels square area in a single 300 pixels by 300 pixels image with a stride of (1,1) overlap. Padding might be needed since 600 doesn't divide evenly by 36. “Valid” padding means that there won’t be additional padding around the borders of the input image whereas "same" padding means padding around the input to create the same dimensions as the output filters. “Same” padding drastically increases the parameters of the network. For instance, a “valid” padding with two convolutional layers and a 3-layer perceptron has 5,063,428 parameters while the “same” padding of the equivalent architecture results in 9,623,422 parameters. The rectifier activation function outputs from the first layer of the convolutional neural network. It is an easier option than the sigmoid activation function because the rectifier activation function max(0, x) creates nonlinearity with the summation of weights without as much computation as the sigmoid activation function.<br>
<tab2>After the 6 pixels by 6 pixels convolutions are individually formed and activated with the rectifier, the 6 pixels by 6 pixels pool size will down-sample the number of pixels per output filter for the first convolutional layer. Then, a second two-dimensional convolutional neural layer is added to the Sequential model. To enable a growth of learning, the number of output filters is increased twice as much from 128 in the first layer to 256 in the second convolutional layer. The stride is still (1,1) by default in Keras with a convolution size of (6,6) with no padding. The relu activation is also the summation output from the second convolutional layer. After the convolutional filters are created in the second layer, the pixels will be downsampled through maxpool. Maxpooling determines whether each pixel is similar or belong to the surrounding pixels. If it is, the pixel will be visible in the down-sampled layer whereas the pixel removed in the maxpool layer if not. <br>
<tab2>After the formation of two convolutional layers with maxpool in between, the flattening of these layers allows the convolutional network to be fed into a traditional fully connected neural network. Keras requires that the flattening of the pixelized data input to have a defined shape in the format (width, height, color). The drawback of using convolutional layers is that the images must be the same size. However, convolutional layers are capable of extracting pixels and characteristics from the image data and processes them in layered filters. The basic multi-layer perceptron is not capable of directly extracting features from pixels. In the fully connected layer of the network, a Dense layer is added onto the Sequential model. A Dense layer represents a basic perceptron where 512 neurons are fully connected from the flattening layer to the output of the first Dense layer. The output again will be summed and calculated by the rectifier function. The dropout at the rate of 0.35 is added for the neural network to prevent overfitting. The neurons will be dropped out from the network, removing their connective weight functionality from the next Dense layer of the network. <br>
<tab2>The final layer will have four units because the number of output classes is four. The activation function for the last layer will be softmax, which squashes the probabilities of the output classes from zero to one. After the architecture of the neural network is defined as a Sequential model, the convolutional and dense layers for the single-label output with multi-class classification will be compiled. The optimizer for compiling the convolutional neural network is one of the most crucial parameters of the learning process. This is because the optimizer determines the learning rate, decay and momentum of the network for correct weight updates during the forward-pass and backpropogation.</p> 


</div>
</div>

  <div class="column side">
  <div style="width:300px;height:700px;line-height:1.2em;overflow:auto;padding:0px;">
    <h2>Questions or Comments?</h2>
    <p><a href="mailto:maggie.cao.bc@gmail.com" style="text-decoration:none;">Email Me</a> 
   </p>
  </div>
</div>

</body>
</html>
